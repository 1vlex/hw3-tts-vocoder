# Отчет по HW3 TTS Vocoder

## 1. Выбранная модель

- **Статья:** HiFi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis (2020)
- **Почему выбрал:** из предложенных вариантов HiFi-GAN является наиболее удобным для базовой реализации вокодера с нуля. Архитектура хорошо подходит для учебного задания: понятная схема генератора (mel -> waveform), стандартный набор discriminator'ов и широко используемые loss-функции.

---

## 2. Архитектура

### Generator
Реализован HiFi-GAN-подобный генератор:
- вход: log-mel спектрограмма (`[B, 80, T]`)
- последовательность upsampling-блоков (transposed conv)
- сверточные residual-блоки между этапами апсемплинга
- выход: waveform (`[B, 1, T_audio]`)

Использовалась упрощенная baseline-конфигурация (без дополнительных архитектурных улучшений).

### Discriminators
Использован набор дискриминаторов в духе HiFi-GAN:
- **Multi-Period Discriminator (MPD)** - несколько discriminator'ов по разным периодам
- **Multi-Scale Discriminator (MSD)** - несколько discriminator'ов на разных временных масштабах

Это позволяет лучше контролировать как локальную структуру сигнала, так и периодичность речевого/гармонического содержимого.

### Лоссы
Использовалась стандартная комбинация:
- **Adversarial loss** (для генератора и дискриминаторов)
- **Feature Matching loss**
- **Mel reconstruction loss** (L1 по mel-признакам между оригиналом и синтезом)

Именно mel-loss дает быстрый прогресс в начале обучения, а adversarial + feature matching улучшают perceptual quality.

### Mel-конфиг
Использовалась mel-конфигурация (совместимая в train/infer):
- `sample_rate = 22050`
- `n_fft = 1024`
- `win_length = 1024`
- `hop_length = 256`
- `n_mels = 80`
- `f_min = 0`
- `f_max = 11025`
- log-mel через `clamp + log`

---

## 3. Данные

### Датасет
Использован датасет **RUSLAN** (русская речь) для обучения вокодера.

### Сплиты
В baseline-версии использовался упрощенный режим:
- train/val на основе одной директории с `.wav` (без отдельного фиксированного сплита по метаданным)

Это допустимо для базовой проверки реализации и получения рабочего вокодера, но для более строгой оценки желательно формировать отдельный train/val split.

### Препроцессинг
- загрузка `.wav`
- приведение к mono
- (при необходимости) ресемплинг
- извлечение log-mel спектрограмм
- случайная нарезка сегментов waveform фиксированной длины для обучения

### Ресемплинг
Выбран режим **22050 Hz**:
- снижает вычислительную нагрузку
- стандартный и удобный вариант для TTS/vocoder baseline
- позволяет быстрее обучать и проверять инференс

---

## 4. Эксперименты

### Baseline
Реализована и обучена упрощенная baseline-версия HiFi-GAN-подобного вокодера:
- обучение с нуля (без предобученных вокодеров)
- adversarial + feature matching + mel loss
- инференс через `synthesize.py`
- поддержка режима **resynthesis** (ground-truth audio -> mel -> vocoder -> audio)

Результат baseline:
- речь разборчива
- интонация в целом сохраняется
- присутствует заметный роботизированный оттенок голоса

### Улучшение 1
В финальную версию дополнительные архитектурные улучшения не добавлялись.
Фокус был на:
- рабочей реализации baseline
- воспроизводимости
- корректном инференсе
- Colab demo
- логировании эксперимента в CometML

### Улучшение 2
Не проводилось.

---

## 5. Логи обучения

- Логирование: **CometML**
- Ссылка на эксперимент:
  - https://www.comet.com/1vlex/hw3-tts-vocoder/9530bf5d8ca04618abf110639234bbff?compareXAxis=step&experiment-tab=panels&showOutliers=true&smoothing=0&viewId=new&xAxis=epoch

### Наблюдения по логам
- `train/loss_mel` быстро снижается в начале обучения, затем выходит на плато
- `val/mel_l1` улучшается на раннем этапе, после чего прогресс замедляется
- adversarial/discriminator loss стабилизируются, но субъективное качество перестает заметно расти после нескольких эпох

### Время и ресурсы
- Локальная машина: **Windows**, GPU **RTX 5070 Ti (16 GB)**
- По наблюдениям:
  - ~4-5 ГБ VRAM при выбранных настройках
  - порядка ~20 минут на эпоху (в baseline-конфигурации)

---

## 6. Анализ качества

### 6.1 На обучающих данных (resynthesis, 15 файлов)

Проведен качественный анализ **15 файлов** из RUSLAN в режиме **resynthesis**:
- исходное аудио -> mel -> вокодер -> синтезированное аудио

#### Агрегированная качественная оценка (0..2)
- **Разборчивость:** 2.0
- **Натуральность:** 1.0
- **Роботизированность (штраф):** 2.0
- **Артефакты (штраф):** 0.5
- **Интонация:** 2.0

#### Слуховые наблюдения
- текст в большинстве случаев хорошо различим
- слова понятны и распознаются без существенных проблем
- базовая интонация и ритм фразы в целом сохраняются
- основной артефакт - **роботизированный/металлический оттенок** тембра
- артефакты присутствуют, но не являются главным ограничением разборчивости (по сравнению с тембром)

#### Вывод по обучающим данным
Baseline-модель уже решает задачу восстановления речи из mel-спектрограмм на приемлемом уровне разборчивости, но качество тембра и натуральность звучания ограничены.

---

### 6.2 На внешних данных (MOS 1.wav, 2.wav, 3.wav)

Для оценки вокодера использовались ground-truth аудио из набора MOS (как требуется в задании), в режиме **resynthesis**.

#### Наблюдения
- речь остается понятной
- длинные фразы воспроизводятся без полного развала структуры
- интонационный контур в целом сохраняется
- роботизированный оттенок сохраняется и заметен на слух

#### Сравнение с обучающими данными
Выводы в целом совпадают с анализом на RUSLAN:
- разборчивость приемлемая
- натуральность тембра ограничена
- на внешних данных артефакты субъективно могут восприниматься сильнее

#### Вывод по внешним данным
Модель пригодна для демонстрации базового vocoder pipeline и resynthesis, но для высокого MOS требуется усиление архитектуры и/или более качественная настройка обучения.

---

### 6.3 Анализ временной и частотно-временной областей

Для качественного анализа использовалось сравнение:
- waveform (оригинал vs синтез)
- log-mel представлений (оригинал vs синтез)

#### Наблюдения по waveform
- временная структура сигнала в целом сохраняется: участки речи и паузы совпадают по расположению
- макродинамика амплитуды похожа на оригинал
- у сгенерированного сигнала заметна большая шероховатость/искусственность формы, что согласуется со слуховым ощущением роботизированности

#### Наблюдения по log-mel
- общая спектральная структура речи сохраняется
- формантная/энергетическая картина близка к оригиналу на уровне общей структуры
- различия проявляются в деталях и "гладкости" спектральной картины, что отражается на тембре и натуральности

---

## 7. Что сработало / что не сработало

### Что сработало
- Реализация baseline vocoder (HiFi-GAN-подобный) обучается с нуля и работает в режиме resynthesis
- `synthesize.py` корректно сохраняет выходные аудиофайлы
- Реализован `CustomDirDataset` под формат `audio/ + transcriptions/`
- Подготовлен и проверен **Google Colab demo**
- Настроено логирование в **CometML**
- Добавлен скрипт скачивания чекпоинта (`download_checkpoints.py`), включая вариант загрузки с Google Drive

### Что не сработало / ограничения
- Натуральность звука осталась на baseline-уровне (заметный роботизированный оттенок)
- После нескольких эпох субъективное качество перестало заметно улучшаться
- Не проводились архитектурные улучшения / абляции
- Полный TTS pipeline (внешняя акустическая модель + вокодер) в финальную версию отчета не включался

---

## 8. Основные сложности

- Сборка воспроизводимого пайплайна: локальное обучение + Colab инференс
- Корректная загрузка чекпоинта через скрипт (в т.ч. Google Drive)
- Структура проекта и импорты для запуска в Colab
- Подбор конфигурации, которая стабильно обучается и укладывается в доступные ресурсы
- Достижение perceptual quality: разборчивость достигается быстрее, чем натуральность тембра

---

## 9. Воспроизводимость

Подробные команды запуска обучения и инференса вынесены в основной `README.md` репозитория.

В репозитории также приложен Colab-ноутбук:
- `demo/demo_colab.ipynb`

Он демонстрирует:
- установку зависимостей
- скачивание чекпоинта
- подготовку MOS-данных
- запуск `synthesize.py`
- прослушивание результатов

---

## Приложение: иллюстрации анализа

### Сводный СУБЪЕКТИВНЫЙ профиль качества (агрегированно, 15 файлов)
![Сводный профиль качества](https://github.com/1vlex/hw3-tts-vocoder/blob/main/report/figures/quality_radar_aggregate.png)

### Сравнение waveform на MOS-примере
![Waveform MOS example](https://github.com/1vlex/hw3-tts-vocoder/blob/main/report/figures/mos1_waveforms.png)

### Сравнение waveform на примере из RUSLAN
![Waveform RUSLAN example](https://github.com/1vlex/hw3-tts-vocoder/blob/main/report/figures/ruslan_example_waveforms.png)

### Сравнение log-mel на MOS-примере
![Mel MOS example](https://github.com/1vlex/hw3-tts-vocoder/blob/main/report/figures/mos1_mels.png)
